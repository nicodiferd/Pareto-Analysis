{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flora Data Cleaning - January 2026 Extract\n",
    "\n",
    "Cleans `flora_data_01_2026.csv` which has two formats:\n",
    "1. **JSON wrapped**: `{\"messages\": [...]}` with multi-layer escaping\n",
    "2. **Plain text**: Direct message content\n",
    "\n",
    "**Input:** `flora_data_01_2026.csv` (356 rows, ~13.8MB)  \n",
    "**Output:** `flora_data_01_2026_cleaned.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_FILE = DATA_DIR / 'flora_data_01_2026.csv'\n",
    "CLEANED_FILE = DATA_DIR / 'flora_data_01_2026_cleaned.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(RAW_FILE, encoding='utf-8-sig')\n",
    "print(f\"Loaded {len(df_raw)} rows\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"Raw file size: {RAW_FILE.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing Functions\n",
    "\n",
    "Handles two data formats and multi-layer JSON escaping:\n",
    "- `\\\"` (1 backslash + quote) = JSON structure quotes\n",
    "- `\\\\\\\"` (3 backslashes + quote) = nested content quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_outer_quotes(val):\n",
    "    \"\"\"Remove outer quotes from CSV fields.\"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        return val\n",
    "    val = val.strip()\n",
    "    if val.startswith('\"') and val.endswith('\"') and len(val) > 2:\n",
    "        return val[1:-1]\n",
    "    return val\n",
    "\n",
    "\n",
    "def parse_input_field(val):\n",
    "    \"\"\"Parse input - handles JSON and plain text formats.\"\"\"\n",
    "    if pd.isna(val) or not isinstance(val, str):\n",
    "        return {'prompt': None, 'format': 'empty'}\n",
    "    \n",
    "    stripped = strip_outer_quotes(val)\n",
    "    \n",
    "    if stripped.startswith('{'):\n",
    "        # JSON format - handle multi-layer escaping\n",
    "        unescaped = stripped.replace('\\\\\\\\\\\"', '___NESTED___')  # 3 backslashes + quote\n",
    "        unescaped = unescaped.replace('\\\\\"', '\"')                 # 1 backslash + quote  \n",
    "        unescaped = unescaped.replace('___NESTED___', '\\\\\"')      # restore nested\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(unescaped)\n",
    "            if 'messages' in data:\n",
    "                for msg in data['messages']:\n",
    "                    if msg.get('type') == 'human' or msg.get('role') == 'user':\n",
    "                        content = msg.get('content', '')\n",
    "                        if isinstance(content, list):\n",
    "                            content = ' '.join([c.get('text', '') for c in content if isinstance(c, dict)])\n",
    "                        return {'prompt': content, 'format': 'json'}\n",
    "            return {'prompt': None, 'format': 'json_no_human'}\n",
    "        except json.JSONDecodeError:\n",
    "            return {'prompt': stripped, 'format': 'json_failed'}\n",
    "    else:\n",
    "        return {'prompt': stripped, 'format': 'plain'}\n",
    "\n",
    "\n",
    "def parse_output_field(val):\n",
    "    \"\"\"Parse output - extracts response, model, and token metadata.\"\"\"\n",
    "    result = {\n",
    "        'response': None, 'model': None, 'model_provider': None,\n",
    "        'input_tokens': None, 'output_tokens': None, 'total_tokens': None,\n",
    "        'stop_reason': None, 'format': 'empty'\n",
    "    }\n",
    "    \n",
    "    if pd.isna(val) or not isinstance(val, str):\n",
    "        return result\n",
    "    \n",
    "    stripped = strip_outer_quotes(val)\n",
    "    \n",
    "    if stripped.startswith('{'):\n",
    "        unescaped = stripped.replace('\\\\\\\\\\\"', '___NESTED___')\n",
    "        unescaped = unescaped.replace('\\\\\"', '\"')\n",
    "        unescaped = unescaped.replace('___NESTED___', '\\\\\"')\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(unescaped)\n",
    "            result['format'] = 'json'\n",
    "            \n",
    "            if 'messages' in data:\n",
    "                for msg in reversed(data['messages']):\n",
    "                    if msg.get('type') == 'ai':\n",
    "                        content = msg.get('content', '')\n",
    "                        if isinstance(content, list):\n",
    "                            content = ' '.join([c.get('text', '') for c in content if isinstance(c, dict)])\n",
    "                        result['response'] = content\n",
    "                        \n",
    "                        resp_meta = msg.get('response_metadata', {})\n",
    "                        result['model'] = resp_meta.get('model_name')\n",
    "                        result['model_provider'] = resp_meta.get('model_provider')\n",
    "                        result['stop_reason'] = resp_meta.get('stop_reason')\n",
    "                        \n",
    "                        usage = msg.get('usage_metadata', {})\n",
    "                        result['input_tokens'] = usage.get('input_tokens')\n",
    "                        result['output_tokens'] = usage.get('output_tokens')\n",
    "                        result['total_tokens'] = usage.get('total_tokens')\n",
    "                        break\n",
    "            return result\n",
    "        except json.JSONDecodeError:\n",
    "            result['response'] = stripped\n",
    "            result['format'] = 'json_failed'\n",
    "            return result\n",
    "    else:\n",
    "        result['response'] = stripped\n",
    "        result['format'] = 'plain'\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process input fields\n",
    "print(\"Processing input fields...\")\n",
    "input_results = df_raw['input'].apply(parse_input_field)\n",
    "df_raw['prompt'] = input_results.apply(lambda x: x['prompt'])\n",
    "df_raw['input_format'] = input_results.apply(lambda x: x['format'])\n",
    "\n",
    "# Process output fields  \n",
    "print(\"Processing output fields...\")\n",
    "output_results = df_raw['output'].apply(parse_output_field)\n",
    "df_raw['response'] = output_results.apply(lambda x: x['response'])\n",
    "df_raw['model'] = output_results.apply(lambda x: x['model'])\n",
    "df_raw['model_provider'] = output_results.apply(lambda x: x['model_provider'])\n",
    "df_raw['input_tokens'] = output_results.apply(lambda x: x['input_tokens'])\n",
    "df_raw['output_tokens'] = output_results.apply(lambda x: x['output_tokens'])\n",
    "df_raw['total_tokens'] = output_results.apply(lambda x: x['total_tokens'])\n",
    "df_raw['stop_reason'] = output_results.apply(lambda x: x['stop_reason'])\n",
    "df_raw['output_format'] = output_results.apply(lambda x: x['format'])\n",
    "\n",
    "print(f\"\\nExtraction Results:\")\n",
    "print(f\"  Prompts: {df_raw['prompt'].notna().sum()}/{len(df_raw)}\")\n",
    "print(f\"  Responses: {df_raw['response'].notna().sum()}/{len(df_raw)}\")\n",
    "print(f\"  Models: {df_raw['model'].notna().sum()}/{len(df_raw)}\")\n",
    "print(f\"\\nInput formats: {df_raw['input_format'].value_counts().to_dict()}\")\n",
    "print(f\"Output formats: {df_raw['output_format'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean String Columns & Parse Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string columns\n",
    "string_cols = ['id', 'timestamp', 'name', 'userId', 'sessionId', 'environment']\n",
    "for col in string_cols:\n",
    "    if col in df_raw.columns:\n",
    "        df_raw[col] = df_raw[col].apply(strip_outer_quotes)\n",
    "\n",
    "# Parse timestamp\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "\n",
    "# Date features\n",
    "df_raw['date'] = df_raw['timestamp'].dt.date\n",
    "df_raw['hour'] = df_raw['timestamp'].dt.hour\n",
    "df_raw['day_of_week'] = df_raw['timestamp'].dt.day_name()\n",
    "df_raw['week'] = df_raw['timestamp'].dt.isocalendar().week\n",
    "\n",
    "print(f\"Date range: {df_raw['date'].min()} to {df_raw['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length features\n",
    "df_raw['prompt_length'] = df_raw['prompt'].fillna('').str.len()\n",
    "df_raw['response_length'] = df_raw['response'].fillna('').str.len()\n",
    "df_raw['prompt_words'] = df_raw['prompt'].fillna('').str.split().str.len()\n",
    "df_raw['response_words'] = df_raw['response'].fillna('').str.split().str.len()\n",
    "\n",
    "# Detect embedded data (JSON in prompts)\n",
    "def has_embedded_data(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    return bool(re.search(r'[{\\[].*[}\\]]', text, re.DOTALL))\n",
    "\n",
    "df_raw['has_embedded_data'] = df_raw['prompt'].apply(has_embedded_data)\n",
    "\n",
    "# Session sequence\n",
    "df_raw = df_raw.sort_values(['sessionId', 'timestamp'])\n",
    "df_raw['session_msg_num'] = df_raw.groupby('sessionId').cumcount() + 1\n",
    "df_raw['is_first_in_session'] = df_raw['session_msg_num'] == 1\n",
    "\n",
    "# Simplify model names\n",
    "def simplify_model_name(model):\n",
    "    if pd.isna(model):\n",
    "        return 'unknown'\n",
    "    model = str(model).lower()\n",
    "    if 'opus' in model: return 'claude-opus'\n",
    "    elif 'sonnet' in model: return 'claude-sonnet'\n",
    "    elif 'haiku' in model: return 'claude-haiku'\n",
    "    elif 'gpt-4.5' in model: return 'gpt-4.5'\n",
    "    elif 'gpt-4o' in model: return 'gpt-4o'\n",
    "    elif 'gpt-4o-mini' in model: return 'gpt-4o-mini'\n",
    "    elif 'gpt-4o-2024-08-06' in model: return 'gpt-4o-08'\n",
    "    elif 'gpt-4' in model: return 'gpt-4'\n",
    "    elif 'gpt-3' in model: return 'gpt-3.5'\n",
    "    return model\n",
    "\n",
    "df_raw['model_simple'] = df_raw['model'].apply(simplify_model_name)\n",
    "\n",
    "print(f\"Embedded data in prompts: {df_raw['has_embedded_data'].sum()} ({df_raw['has_embedded_data'].mean()*100:.1f}%)\")\n",
    "print(f\"Unique sessions: {df_raw['sessionId'].nunique()}\")\n",
    "print(f\"\\nModel distribution:\")\n",
    "print(df_raw['model_simple'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Clean DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_columns = [\n",
    "    'id', 'timestamp', 'date', 'hour', 'day_of_week', 'week',\n",
    "    'userId', 'sessionId', 'session_msg_num', 'is_first_in_session',\n",
    "    'prompt', 'response', 'has_embedded_data',\n",
    "    'prompt_length', 'response_length', 'prompt_words', 'response_words',\n",
    "    'model', 'model_simple', 'model_provider',\n",
    "    'input_tokens', 'output_tokens', 'total_tokens', 'stop_reason',\n",
    "    'input_format', 'output_format',\n",
    "    'name', 'environment'\n",
    "]\n",
    "\n",
    "df_clean = df_raw[clean_columns].copy()\n",
    "df_clean = df_clean.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f\"Clean dataset: {len(df_clean)} rows, {len(df_clean.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOVERVIEW\")\n",
    "print(f\"  Total records: {len(df_clean)}\")\n",
    "print(f\"  Date range: {df_clean['date'].min()} to {df_clean['date'].max()}\")\n",
    "print(f\"  Unique users: {df_clean['userId'].nunique()}\")\n",
    "print(f\"  Unique sessions: {df_clean['sessionId'].nunique()}\")\n",
    "\n",
    "print(f\"\\nEXTRACTION SUCCESS\")\n",
    "print(f\"  Prompts: {df_clean['prompt'].notna().sum()}/{len(df_clean)} ({df_clean['prompt'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"  Responses: {df_clean['response'].notna().sum()}/{len(df_clean)} ({df_clean['response'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"  Models: {df_clean['model'].notna().sum()}/{len(df_clean)} ({df_clean['model'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"  Tokens: {df_clean['total_tokens'].notna().sum()}/{len(df_clean)} ({df_clean['total_tokens'].notna().mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTOKEN USAGE (where available)\")\n",
    "print(f\"  Total tokens: {df_clean['total_tokens'].sum():,.0f}\")\n",
    "print(f\"  Avg per request: {df_clean['total_tokens'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nTOP 5 USERS\")\n",
    "user_counts = df_clean['userId'].value_counts().head(5)\n",
    "cumsum = 0\n",
    "for i, (user, count) in enumerate(user_counts.items(), 1):\n",
    "    cumsum += count\n",
    "    print(f\"  User {i}: {count} msgs ({count/len(df_clean)*100:.1f}%) - cumulative: {cumsum/len(df_clean)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(CLEANED_FILE, index=False)\n",
    "\n",
    "print(f\"Saved: {CLEANED_FILE}\")\n",
    "print(f\"Size: {CLEANED_FILE.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"Compression: {RAW_FILE.stat().st_size / CLEANED_FILE.stat().st_size:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAMPLE PROMPTS\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in df_clean.head(5).iterrows():\n",
    "    prompt = row['prompt']\n",
    "    if prompt:\n",
    "        display = prompt[:200].replace('\\n', ' ')\n",
    "        if len(prompt) > 200:\n",
    "            display += \"...\"\n",
    "        print(f\"\\n[{row['model_simple']}] {row['input_format']}\")\n",
    "        print(f\"{display}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc29m8gkcua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompts (first 150 chars):\n",
      "\n",
      "0: Analyze the following data and provide a concise summary of the key findings. Keep your response in a short paragraph of a few sentences and impactful...\n",
      "\n",
      "1: How did the tasks flow through the statuses...\n",
      "\n",
      "2: Analyze the following data and provide a concise summary of the key findings. Keep your response in a short paragraph of a few sentences and impactful...\n",
      "\n",
      "3: how are my top 25 initiatives doing for deliverability...\n",
      "\n",
      "4: Analyze the following data and provide a concise summary of the key findings. Keep your response in a short paragraph of a few sentences and impactful...\n",
      "\n",
      "5: Show me all of my off-track initiatives....\n",
      "\n",
      "6: Analyze the following data and provide a concise summary of the key findings. Keep your response in a short paragraph of a few sentences and impactful...\n",
      "\n",
      "7: Show me all of my off-track initiatives....\n",
      "\n",
      "8: Analyze data work period 'REBEL-2025 SP26' ...\n",
      "\n",
      "9: provide a performance analysis for work period 'REBEL-2025 SP26'...\n",
      "\n",
      "10: Analyze the following data and provide a concise summary of the key findings. Keep your response in a short paragraph of a few sentences and impactful...\n",
      "\n",
      "11: Im preparing for a sprint retro and want to analyze work period 'STAR-2025 SP23'. Please provide executive summary, key metric performance, Performanc...\n",
      "\n",
      "12: For work period 'STAR-2025 SP23' Provide executive summary, key metric performance and flow of work analysis...\n",
      "\n",
      "13: provide sprint reto analysis for work period 'STAR-2025 SP23'...\n",
      "\n",
      "14: provide sprint reto analysis for work period 'STAR-2025 SP23'...\n",
      "\n",
      "15: for work period 'REBEL-2025 SP26' provide sprint retro analysis...\n",
      "\n",
      "16: for work period 'REBEL-2025 SP26' provide sprint retro analysis...\n",
      "\n",
      "17: What are the key metrics under the Chipotle portfolio for this week?...\n",
      "\n",
      "18: Analyze the following contributing Epic data for this initiative and provide a concise summary of the key findings. This data includes tasks or points...\n",
      "\n",
      "19: Analyze the following data and provide a concise summary of the key findings. Keep your response in a short paragraph of a few sentences and impactful...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check sample prompts to tune intent categorization\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('/Users/nicolodiferdinando/Desktop/School/Semesters/Winter26/IME482/Pareto Analysis/data')\n",
    "df = pd.read_csv(DATA_DIR / 'flora_data_01_2026_cleaned.csv')\n",
    "\n",
    "# Sample of prompts\n",
    "print(\"Sample prompts (first 150 chars):\\n\")\n",
    "for i, prompt in enumerate(df['prompt'].head(20)):\n",
    "    if prompt:\n",
    "        print(f\"{i}: {prompt[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "zcfnq6uxzq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current intent distribution:\n",
      "intent\n",
      "Executive Reporting     118\n",
      "Other                    85\n",
      "Metrics Query            57\n",
      "Information Request      46\n",
      "Data Analysis            17\n",
      "Status Update            10\n",
      "Team Query                9\n",
      "Initiative Query          8\n",
      "Sprint Retrospective      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total: 356\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def categorize_intent(prompt):\n",
    "    \"\"\"Categorize prompt intent based on ACTION + TARGET patterns.\"\"\"\n",
    "    if pd.isna(prompt):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    prompt_lower = prompt.lower()\n",
    "    \n",
    "    # Executive Reporting\n",
    "    if re.search(r'(provide|generate|create|give).*(summary|report|executive|overview)', prompt_lower):\n",
    "        return 'Executive Reporting'\n",
    "    \n",
    "    # Data Analysis\n",
    "    if re.search(r'analyze.*(data|following|metrics|phase|cards)', prompt_lower):\n",
    "        return 'Data Analysis'\n",
    "    \n",
    "    # Sprint Retrospective (including typo \"reto\")\n",
    "    if re.search(r'sprint.*(retro|reto|review|analysis|recap)', prompt_lower):\n",
    "        return 'Sprint Retrospective'\n",
    "    \n",
    "    # Sprint Planning\n",
    "    if re.search(r'sprint.*(plan|prep|upcoming|next)', prompt_lower):\n",
    "        return 'Sprint Planning'\n",
    "    \n",
    "    # Metrics Query\n",
    "    if re.search(r'(velocity|throughput|cycle time|lead time|burndown)', prompt_lower):\n",
    "        return 'Metrics Query'\n",
    "    \n",
    "    # Status Update\n",
    "    if re.search(r'(status|progress|update|where.*stand)', prompt_lower):\n",
    "        return 'Status Update'\n",
    "    \n",
    "    # Information Request\n",
    "    if re.search(r'^(tell me|what is|what are|explain|describe|how)', prompt_lower):\n",
    "        return 'Information Request'\n",
    "    \n",
    "    # Initiative/Project Query\n",
    "    if re.search(r'(initiative|project|epic|feature)', prompt_lower):\n",
    "        return 'Initiative Query'\n",
    "    \n",
    "    # Team Query\n",
    "    if re.search(r'(team|squad|group)', prompt_lower):\n",
    "        return 'Team Query'\n",
    "    \n",
    "    return 'Other'\n",
    "\n",
    "# Apply and check distribution\n",
    "df['intent'] = df['prompt'].apply(categorize_intent)\n",
    "print(\"Current intent distribution:\")\n",
    "print(df['intent'].value_counts())\n",
    "print(f\"\\nTotal: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8owunbrqz8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Other' category prompts:\n",
      "\n",
      "0: provide a performance analysis for work period 'REBEL-2025 SP26'...\n",
      "\n",
      "1: why is my portfolio performance decreasing in december...\n",
      "\n",
      "2: `...\n",
      "\n",
      "3: can you compare december with november data\\n...\n",
      "\n",
      "4: Analyze [@REBEL-Scrum] from 8/1/25 through 12/31/25 and identify where the main bottlenecks, and which workflow states c...\n",
      "\n",
      "5: \"\"...\n",
      "\n",
      "6: show be results across my portfolio for SP26...\n",
      "\n",
      "7: \"\"...\n",
      "\n",
      "8: Can you show Readiness over this sprint...\n",
      "\n",
      "9: what about sprint readiness...\n",
      "\n",
      "10: For @work_period:ef237015-9250-4b1a-9797-6b983f46482b[REBEL-2025 SP26]@parent:board:b1192a4c-f0fb-4fac-aa9a-17b284d95b8e...\n",
      "\n",
      "11: define stale task...\n",
      "\n",
      "12: are there any cards in @work_period:f19de16c-bd76-4113-93ef-cf4fb8930b56[FBI-2026 SP02]@parent:board:c18efa8d-d631-4ef2-...\n",
      "\n",
      "13: Create a chart that shows the Commitment Reliability by Story Type for these sprints @work_period:f19de16c-bd76-4113-93e...\n",
      "\n",
      "14: Retrospective Insights Trend for @work_period:ea1b49f8-77ab-4180-869b-e9702a2e29d8[FBI-2026 SP01]@parent:board:c18efa8d-...\n",
      "\n",
      "15: What was the overall rework rate for PI22 from January 2025 to December 2025?...\n",
      "\n",
      "16: \"\"...\n",
      "\n",
      "17: what caused my sprint to be off track...\n",
      "\n",
      "18: show me the completed points for sprint @work_period:9ad58721-196a-46d8-83ec-bf730119e433[REBEL-2026 SP02]@parent:board:...\n",
      "\n",
      "19: What was the predictability rate of delivery for the ART1 program board?...\n",
      "\n",
      "20: \"\"...\n",
      "\n",
      "21: @linen...\n",
      "\n",
      "22: @linen...\n",
      "\n",
      "23: @linen...\n",
      "\n",
      "24: Review the process map flow for @board:b1192a4c-f0fb-4fac-aa9a-17b284d95b8e[REBEL-Scrum] from 8/1/25 to 12/31/25. Highli...\n",
      "\n",
      "25: review this process map for improvemnts and performance...\n",
      "\n",
      "26: review this process map for improvemnts and performance...\n",
      "\n",
      "27: Analyze process map...\n",
      "\n",
      "28: review this process map for improvemnts and performance...\n",
      "\n",
      "29: @work_period:e4fa1cda-8a18-42de-8882-dcb3e0eb1387[REBEL-2026 SP01]@parent:board:b1192a4c-f0fb-4fac-aa9a-17b284d95b8e[REB...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check \"Other\" prompts to see if we can categorize more\n",
    "other_prompts = df[df['intent'] == 'Other']['prompt'].head(30)\n",
    "print(\"'Other' category prompts:\\n\")\n",
    "for i, prompt in enumerate(other_prompts):\n",
    "    print(f\"{i}: {prompt[:120]}...\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
